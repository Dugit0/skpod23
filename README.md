# СКиПОДы. 5 семестр. ВМК МГУ. Вариант 20.
Если вы нашли этот репозиторий и у вас 20-й вариант - поздравляю!
Дальше следует информация, которую нужно знать о данном варианте и о данном репозитории для успешного взаимодействия с ним.

## Формулировка задания
Для выполнения практических заданий используются суперкомпьютерные вычислительные ресурсы факультета ВМК.
http://hpc.cs.msu.ru/

В каждой задаче требуется:
1. Для предложенного алгоритма реализовать несколько версий параллельных программ с использованием технологии OpenMP.
    1. Вариант параллельной программы с распределением витков циклов при помощи директивы `for`.
    2. Вариант параллельной программы с использованием механизма задач (директива `task`).
    2. Для программ с регулярной зависимостью по данным вместо механизма задач допускается реализация и сравнение различных версий конвейерного выполнения циклов, параллелизм по гиперплоскостям и др.

2. Реализовать параллельную версию программы с использованием технологии MPI.

3. Убедиться в корректности разработанных версий программ.

4. Начальные параметры для задачи должны быть подобраны таким образом, чтобы:
    1. Задача помещалась в оперативную память одного узла кластера.
    2. Время решения задачи было в примерном диапазоне 5 сек.-15 минут.

5. Исследовать эффективность полученных параллельных программ на суперкомпьютере Polus.
    1. Сравнить варианты разработанных версий параллельных программ.
    2. Если в процессе распараллеливания программа была существенно оптимизирована/изменена/переписана (например, на С++) необходимо провести сравнение - исходная программа VS программа после преобразований.
    3. Исследовать влияние различных опций оптимизации, которые поддерживаются компиляторами (`-O2`, `-O3`, `-fast`...)

6. Исследовать масштабируемость полученной параллельной программы: построить графики зависимости времени выполнения параллельной программы от числа используемых ядер для различного объёма входных данных.
Оптимальным является построение трёхмерного графика: по одной из осей время работы программы, по другой - количество ядер и по третьей - объём входных данных.
Такой график необходимо построить для каждого из разработанных вариантов программы.
Каждый прогон программы с новыми параметрами рекомендуется выполнять несколько раз с последующим усреднением результата (для избавления от случайных выбросов).
Для замера времени рекомендуется использовать функцию `omp_get_wtime`, общее время работы должно определяться временем работы самой медленной нити/процесса.
Количество ядер/процессоров рекомендуется задавать в виде $p=1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 40, 60, 80, 100, 120, 140, 160$.

7. Определить основные причины недостаточной масштабируемости программы при максимальном числе используемых ядер/процессоров.

8. Подготовить отчет о выполнении задания, включающий: описание реализованного алгоритма, графики зависимости времени исполнения от числа ядер/процессоров для различного объёма входных данных, текст программы.

## Структура репозитория
- В папке `base_prog` лежат исходные коды решения.
- В папке `latex_report` лежат файлы отчета в формате `.tex`.
- В папке `polus_results` лежат отчеты запусков на суперкомпьютере Polus.
**Внимание! Запуски производились некорректно!** (См. "О запуске на Polus")
## Комментарии к решениям
### О компиляции
Для компиляции OpenMP-программ использовалась следующая функция, которую следует поместить в `.bashrc`
```
function cmcgcc_omp() {
    outfile=${1%%.*}
    echo ${outfile}
    gcc -fsanitize=address,undefined,signed-integer-overflow,pointer-compare,pointer-subtract,leak,bounds,pointer-overflow -fopenmp -O2 -std=c99 -Wall -Wextra -Werror=vla -lm -o "${outfile}.out" ${1}
}
```
Для компиляции MPI-программ, соответственно:
```
function cmcmpi() {
    outfile=${1%%.*}
    echo ${outfile}
    mpicc -Wall -Wextra -fsanitize=address,undefined,signed-integer-overflow,pointer-compare,pointer-subtract,leak,bounds,pointer-overflow -o "${outfile}.out" ${1}
}
```
Для запуска MPI-программ используется команда `mpirun -np 8 ./mpi.out` (флаг `-np` задает количество MPI-нитей).
### О запуске на Polus
Чтобы корректно протестировать программу на суперкомпьютере необходимо захватывать целый кластер, состоящий из 20 ядер, т.к. в противном случае может произойти ситуация, когда на одном кластере окажутся 2 или более человек и запросят большое количество нитей. В этом случае программа может выполняться в десятки раз медленнее.
### Об OpenMP
- В программе `openmp_for.c` используются возможности стандарта OpenMP 4.5, но та реализация, которая установлена на Polus, мегахреново работает и замедляет программу в несколько раз.
- В программе `openmp_redblack.c` используется алгоритм RedBlack. Если вы думаете, как он работает, то - "Никак". Данный алгоритм не работает, но его легко реализовать, он дает огромное ускорение (именно по той причине, что он не работает) и его принимает Бахтин.
- В программе `openmp_hyperplain.c` используется алгоритм распараллеливания по гиперплоскостям (подробнее см. в отчете).
### О MPI
В MPI используется разбиение на блоки, каждый из которых отдается какой-либо MPI-нити и выполняется в ней. Я потратил несколько суток на написание данного кода. Возможно были реализации гораздо проще. Для понимания работы программы посмотрите файлы `scheme1.png` и `scheme2.png`.
### Предупреждение
Т.к. код на MPI я писал довольно долго и он получился "своеобразным" Бахтин вполне мог запомнить эту реализацию. Пользуйтесь с осторожностью!

Удачи!
